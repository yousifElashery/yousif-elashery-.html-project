<html>
<body>
<h1>problems of Artifical intelligence</h1>

<h2> links: </h2>
<ul>
  <li><a href="mainpage.html">main page</a></li>
  <li><a href="typesofartificalintelligence.html">Artifical intelligence</a></li>
  <li><a href="problemsofai.html">problems of Artifical intelligence</a></li>
  <li><a href="AIresearchtools.html">AI research tools</a></li>
  <li><a href="AIapplications.html">AI applications</a></li>
</ul>


In artificial intelligence (AI) and philosophy, the AI control problem is the issue of how to build a superintelligent agent that will aid its creators, and avoid inadvertently building a superintelligence that will harm its creators. Its study is motivated by the notion that humanity will have to solve the control problem before any superintelligence is created, as a poorly designed superintelligence might rationally decide to seize control over its environment and refuse to permit its creators to modify it after launch.[1] In addition, some scholars argue that solutions to the control problem, alongside other advances in AI safety engineering,[2] might also find applications in existing non-superintelligent AI.[3]

Major approaches to the control problem include alignment, which aims to align AI goal systems with human values, and capability control, which aims to reduce an AI system's capacity to harm humans or gain control. Capability control proposals are generally not considered reliable or sufficient to solve the control problem, but rather as potentially valuable supplements to alignment efforts.[1]

</body>
</html>